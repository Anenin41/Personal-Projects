% Section about the ansatz and the Gram-Charlier A series %
% Author: Konstantine Garas
% E-mail: kgaras041@gmail.com // k.gkaras@student.rug.nl
% Created: Wed 19 Feb 2025 @ 17:10:33 +0100
% Modified: Fri 21 Feb 2025 @ 18:22:38 +0100

\section{Building the Ansatz}
\label{sec: ansatz}

This section introduces the mathematical tools that will be needed in modifying the Black-Scholes model. The discussion starts by introducing the key concepts behind the model.

Let $\mathcal{X}(t)$ be a real-valued random variable that depends on time. This mathematical construct (that is also known as a stochastic process) is often used to model a variety of physical or artificial phenomena that change randomly over time. Due to their nature, stochastic processes are commonly used in the stock market as a means to analyze the future behaviour of stock prices or other complex financial assets. 

The probability of a time-dependent random variable to take a value less than, or equal to \( x \), is given by its cumulative distribution function (CDF). This function, is defined as follows.

\[
F_{\mathcal{X}(t)}(x,t) = \mathbb{P}(\mathcal{X}(t) \leq x)
\]

From this CDF, one can also produce the probability density function (PDF) of the random variable by applying the derivative operator.

\[
f_{\mathcal{X}(t)}(x,t) = \frac{\partial F_{\mathcal{X}(t)}(x,t)}{\partial x}
\]

A direct result from probability theory is that the PDF basically expresses the probability of the time-dependent random variable to take values within the range of $(a,b]$. Mathematically this is formulated by an integral.

\[
\mathbb{P}(a < \mathcal{X}(t) \leq b) = F_{\mathcal{X}(t)} (b,t) - F_{\mathcal{X}(t)}(a,t) = \int_{a}^{b} f_{\mathcal{X}(t)} (x,t) dx
\]

In this setting, the CDF and PDF functions are unknown since we don't have the information of how the financial asset will evolve over time. As such, the is the need to approximate them in some way. This is done by using the \textbf{Gram-Charlier A series} described in \cite{wallace1958asymptotic}. Before the introduction of the Method of Moments however, some mathematical preliminaries are required.

\subsection{Preliminaries}
The \textbf{Gram-Charlier A series} uses the \textit{characteristic function} of the distribution of a random variable and expands it based on its \textit{cumulants}, with both concepts being thoroughly analyzed in \cite{blitzstein2019introduction}. Generally, the characteristic function is given by the following form.

\[
\phi_{\mathcal{X}(t)} (t) = \text{E}[e^{it \mathcal{X}(t)}]
\]

It is worthwhile to mention here, that what statisticians call the characteristic function of a distribution, is actually nothing more that its \textit{Fourier Transform}. Fourier Transformations will be used extensively later, to derive the final formula of the Gram-Charlier expansion.

In addition, the expansion also uses the cumulants of a random variable. Cumulants are mathematical terms that are closely related to the moments of a distribution function, and like moments, they are generated by a \textit{generation function}.

\[
	K_{\tau}(t) = \log(\text{E}[e^{t \mathcal{X}(\tau)}])
\]

\begin{remark}
	In the context that is described in this report, $\mathcal{X}(t)$ is a stochastic process. There are many different ways to classify such mathematical objects, which actually depend on the time set, the index set or dependence among the random variables. To be more specific, depending on the cardinality of the time set, the stochastic processes are categorized and studied in a different manner. 
	In the case of this report, such intricate mathematical theory is not needed, since the time set is small and countable. For example under Section \ref{sec: application}, the time of maturity of the European call option will be \(T = 2\). So, the time set \(\mathcal{T} = \{0,1,2\} \), where $t=0$ denotes the current year, and $t=2$ is for example 2 years into the future.
\end{remark}

Because $\mathcal{X}_t$ is completely unknown, it is not possible to calculate its characteristic functions or its cumulants, due to lack of information. However, through the properties of those functions, one can build an approximation to the unknown CDF and PDF, which will later be used in the Gram-Charlier expansion.

By stabilizing the time step of the stochastic process and expanding the cumulant-generating function using the MacLaurin series around zero, the following power series emerges.

\[
K(t) = \sum_{n=1}^{\infty} \kappa_{n} \frac{t^n}{n!}
\]

Using this equation, the $n$-th cumulant of $\mathcal{X}_t$ is given by derivation of the MacLaurin series $n$ times at $t=0$, since \( K(t) \) is a polynomial function. 

It has already been mentioned that the cumulants are closely related to the moments of a distribution function, in the sense that they allow statisticians to study core properties of said distribution. More specifically, the first 4 cumulants, which are linked to the most important moments of a distribution, are defined by the following expressions.


\begin{align*}
	\kappa_{1} (\mathcal{X}_t) &= \text{E}[\mathcal{X}_t]	\\
	\kappa_{2} (\mathcal{X}_t) &= \text{Var}[\mathcal{X}_t] \\
	\kappa_{3} (\mathcal{X}_t) &= \text{E}[(\mathcal{X}_t - \text{E}[\mathcal{X}_t])^3]\\
	\kappa_{4} (\mathcal{X}_t) &= \text{E}[(\mathcal{X}_t - \text{E}[\mathcal{X}_t])^4]- 3 ( \text{E}[(\mathcal{X}_t - \text{E}[\mathcal{X}_t])^2])^2
\end{align*}

Notice that the 1st cumulant is the 1st raw moment of the random variable, or \textit{mean}. Moreover, the 2nd cumulant is the 2nd central moment of the random variable, or the \textit{variance}. The 3rd cumulant is the 3rd central moment, and the 4th cumulant is the 4th central moment, minus 3 times the square of the second central moment.

In a probability distribution, the major properties one is interested in is the mean, the variance, the skewness and the kurtosis. These properties are connected with the raw, central, and standardized moments respectively, and as a direct result, they are also connected with the cumulants.

\begin{remark}
	The $n$-th standardized moment of a random variable is nothing more that the $n$-th central moment divided by $\sigma^n$.
\end{remark}

By combining the equations for the first 4 cumulants and the result from the remark above, one gets relations for the skewness and the kurtosis.


\begin{gather*}
	\text{Skew}[\mathcal{X}_t] = \frac{\kappa_3}{\sigma^3} \\
	\text{Kurt}[\mathcal{X}_t] = \frac{\kappa_4 + 3 \sigma^2}{\sigma^4}
\end{gather*}


In probability theory, skewness and kurtosis are the 3rd and 4th standardized moments of a random variable, respectively. Skewness measures the asymmetry of a distribution, while kurtosis characterizes the tail of the distribution (with higher than 3 kurtosis meaning for example a more "fat" tail). Introductory books in probability theory, like \cite{blitzstein2019introduction}, cover the subject of moments thoroughly.

\subsection{Gram-Charlier Expansion}
The idea of the Gram-Charlier A series, is the asymptotic expansion of an unknown distribution function, based on its cumulants, and a known distribution that serves as basis for the expansion.

To this end, let \( \mathcal{X}(t) \) the stochastic process in question, evaluated at constant time, i.e. \( \mathcal{X}_t \). For the basis of the asymptotic expansion I will use the Normal distribution, which is also one of the core assumptions of the Black-Scholes model. As such, let $\hat{f}$ and $\hat{\psi}$ be the characteristic functions of $\mathcal{X}_t$ and $\mathcal{N}(\mu, \sigma^2)$ respectively. Consider $\gamma_r$ the cumulants of the Normal distribution. By once again using the MacLaurin series around zero, the following power series for the Normal characteristic function emerges.

\[
\hat{\psi} = \exp \left( \sum_{r=1}^{\infty} \gamma_r \frac{(it)^r}{r!} \right)
\]

By properties of the exponential function, one can connect the expansion of $\hat{f}$ and $\hat{\psi}$ quite easily.

\[
\hat{f} = \exp \left[ \sum_{r=1}^{\infty} (\kappa_{r} - \gamma_{r}) \frac{(it)^r}{r!} \right] \hat{\psi}(t) \tag{1}
\]

It is worthwhile to notice, that the mathematical construct statisticians call the characteristic function, it is actually nothing more than the \textit{Fourier Transformation} of the probability density function. More specifically, the Fourier Transformation is defined as follows.

\[ 
\mathcal{F} \{ f(x) \} = \int_{-\infty}^{\infty} f(x) e^{-itx} dx
\]

This transformation has very useful mathematical properties. All of the methods that are used below use these widely established characteristics of the Fourier operator, and for reference, they are thoroughly analysed in \cite{herman2015introduction}, on Chapter 9.5. Apart from this, characteristic functions and Fourier transformations are crucial in obtaining the final equation of the \textbf{Gram-Charlier A} series. To this end, I start with the derivative property.

\[
\mathcal{F} \left\{ \frac{d^r}{dx^r} f(x) \right\} = (it)^r \hat{f}(x) \tag{2}
\]

Moreover, Fourier Transforms are bijective operators, meaning that (2) can be inverted.

\begin{gather*}
	\mathcal{F}^{-1} \left\{ \mathcal{F} \left\{ \frac{d^r}{dx^r} \right\} f(x) \right\} = \mathcal{F}^{-1} \{(it)^r \hat{f}(t)\} \implies \\
	\frac{d^r}{dx^r} f(x) = \mathcal{F}^{-1} \{ (it)^r \hat{f}(t) \}
\end{gather*}

By using the result from above in combination with equation (1), a relation including the original characteristic function of the Normal distribution is generated.

\[
	\hat{\psi}(t) = \int_{-\infty}^{\infty} \psi(x) e^{-itx} dx
\]

\[
\mathcal{F} \left\{ \frac{d^r}{dx^r} \psi(x) \right\} = (it)^r \hat{\psi}(t) \tag{3}
\]

A trick is needed to proceed. First, notice simply the following equation.

\[
	\mathcal{F} \{\psi(-x)\}(t) = \int_{-\infty}^{\infty} \psi(-x) e^{-itx}dx = \int_{\infty}^{-\infty} \psi(u) e^{itu} (-du)
\]
by performing a change of variables $u = -x$, the differential also changes to $du = -dx$. However, by taking into consideration that the Fourier Transform is even symmetric (Hermitian Function) for real-valued functions, it is possible to use the time-reversal property of this operator and get equation (4).

\[
\int_{\infty}^{-\infty} \psi(u) e^{itu} d(-u) = \int_{-\infty}^{\infty} \psi(u) e^{-itu}du = \hat{\psi}(-t) \tag{4}
\]

So, $\mathcal{F}\{ \psi(-x) \}(t) = \hat{\psi}(-t)$. Now apply the differential operator $r$ times.

\[
	\frac{d^r}{dx^r} \psi(-x) = (-1)^r \frac{d^r}{d(-x)^r} \psi(-x)
\]

By plugging in this equations on (3), and by using the linearity of the Fourier Transform, an equation arises that calculates the differential of $\psi(-x)$. 

\[
	\mathcal{F} \left\{ (-1)^r \frac{d^r}{d(-x)^r} \psi(-x) \right\} = (-1)^r \mathcal{F} \left\{ \frac{d^r}{d(-x)^r} \psi(-x) \right\} \xRightarrow{(3),(4)} (-1)^r (it)^r \hat{\psi}(t)
\]

Lastly, by setting $r=0$, an equation regarding the original function arises as follows.

\begin{gather*}
	\frac{d^r}{dx^r} f(x) = \mathcal{F}^{-1} \left\{ (it)^r \hat{f} (t) \right\} \xRightarrow[]{r=0} \\
	f(x) = \mathcal{F}^{-1} \left\{ (it)^0 \hat{f}(t) \right\} \implies \\
	f(x) = \mathcal{F}^{-1} \left\{ \hat{f}(t) \right\} \tag{5}
\end{gather*}

It is now time to use all these properties to modify the original expansion. More specifically, use (5), in combination with (4) and (1) to get an expansion regarding the probability density functions, and not the characteristic functions.

\[
	f(-x) = \exp \left[ \sum_{r=1}^{\infty} \frac{\kappa_{r} - \gamma_{r}}{r!} \cdot \left( - \frac{d^r}{d(-x)^r} \right) \right] \psi(-x)
\]

By changing the variable from $-x$ to $x$, one gets the final form of the power series.

\[
	f(x) = \exp \left[ \sum_{r=1}^{\infty} \frac{\kappa_r - \gamma_r}{r!} \cdot \left( - \frac{d^r}{d(-x)^r} \right) \right] \psi(x) \tag{6}
\]

Equation number (6) expands the unknown probability density function with regards to its cumulants and a known probability density function. It is obvious that by choosing different forms of $\psi(x)$, the expansion will be different each time. It was mentioned before that the Normal Distribution will be used to expand the PDF. To this end, let $\psi(x) = \phi(x)$ the PDF of $\mathcal{N}(\mu, \sigma^2)$. Because of the properties of the Normal Distribution, this expansion can be analyzed further.

\begin{remark}
	The first and second cumulants of $\mathcal{N}(\mu, \sigma)$, are the mean and variance, and they are chosen to be equal with the mean and variance of $\mathcal{X}_t$. As such, the first two terms of the power series are zero, because $\kappa_1 = \gamma_1 = \mu$ and $\kappa_2 = \gamma_2 = \sigma^2$. 
	Moreover, all higher order moments of the Normal Distribution are zero! This leads to all the higher order cumulants \( \gamma_r \) with \(r > 2\) being zero as well. This nice observation allows us to mathematically modify the power series into an infinite sum that only contains the cumulants of $\mathcal{X}_t$.
\end{remark}

By changing the power series index from $r=0$ to $r=3$, I can discard the $\gamma_r$ term. 

\[
	f(x) = \exp \left[ \sum_{r=3}^{\infty} \frac{\kappa_r}{r!} \cdot \left( - \frac{d^r}{d(-x)^r} \right) \right] \phi(x) \tag{7}
\]

There exists a really nice recursion formula for the $r$-th derivative of the Normal PDF. This recursive law is introduced in \cite{patel1996handbook}, and connects the derivatives of the PDF with the Hermite polynomials of appropriate degree.

\[
	\frac{d^r}{dx^r} \phi(x) = \phi^{(r)}(x) = \frac{(-1)^r}{\sigma^r} \text{He}_r \left(\frac{x - \mu}{\sigma} \right) \phi(x) \tag{8}
\]
where $\text{He}_r$ is the (probabilistic) Hermite Polynomial of degree $r$.

In addition to all of these results, the cumulants can also be expressed in a combinatorial way using Bell Polynomials. To be more specific, the $n$-th order complete Bell Polynomial is generated by the following expression.

\[ 
	B_n(x_1, x_2, \dots, x_n) = \sum_{1j_1 + 2j_2 + \cdots + nj_n = n} n! \prod_{i=1}^{n} \frac{x_{i}^{j_i}}{(i!)^{j_i} j_i !}
\]
where $j_i$ are non-negative integers satisfying

\[ 
	1j_1 + 2j_2 + \cdots + nj_n = n
\]

By replacing $x_i = \kappa_i$, I can rewrite the expression above in the following way.

\[  
	B_n(\kappa_1, \dots, \kappa_n) = \sum_{1j_1 + \cdots + nj_n = n} \frac{n!}{j_1! j_2! \cdots j_n!} \left( \frac{\kappa_1}{1!} \right)^{j_1} \cdots \, \left( \frac{\kappa_n}{n!} \right)^{j_n} \tag{9}
\]

Now is the time to use the context of the application and shift away from pure mathematical theory. In truth, I am only interested in $\kappa_3$ and $\kappa_4$ since through them, I can study the skewness and kurtosis of the dataset. As such, by applying equations (8) and (9) to (7), I can further transform the ansatz.

\[
	f(x) = \phi(x) \sum_{n=0}^{\infty} \frac{1}{n! \sigma^n} B_n(0, 0, \kappa_3, \dots, \kappa_n) \text{He}_n \left( \frac{x - \mu}{\sigma} \right) \tag{10}
\]

One can also integrate equation (10) to get the CDF of the random variable in question.

\[
	F(x) = \int_{-\infty}^{x} f(u)d(u) = \Phi(x) - \phi(x) \sum_{n=3}^{\infty} \frac{1}{n! \sigma^{n-1}} B_n(0, 0, \kappa_3, \dots, \kappa_n) \text{He}_{n-1} \left( \frac{x-\mu}{\sigma} \right) \tag{11}
\]

These final two equations are the \textbf{Gram-Charlier A series}, and since I am only interested in skewness and kurtosis, I only expand the power series up to $\kappa_4$. This is easily done by taking \(j_1, j_2 = 0\) and \(j_3 = 1 \) to the Bell Polynomial or degree 3, and \(j_1, j_2, j_3 = 0, j_4 = 1\) on the Bell Polynomial of degree 4. Lastly, to make the expansion slightly more visually appealing, a change of variables is performed.

\begin{gather*}
	F(\sigma \xi + \mu) \approx \Phi(\sigma \xi + \mu) - \phi(\sigma \xi + \mu) \sum_{n=3}^{4} \frac{1}{n! \sigma^{n-1}} B_n(0, 0, \kappa_3, \kappa_4) \text{He}_{n-1} (\xi) \\
	f(\sigma \xi + \mu) \approx \phi(\sigma \xi + \mu) \left[ 1 + \frac{\kappa_3}{3! \sigma^3} \text{He}_{3}(\xi) + \frac{\kappa_4}{4!{\sigma^4}} \text{He}_4(\xi) \right]
\end{gather*}
